{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-06T05:26:20.609656Z","iopub.execute_input":"2022-05-06T05:26:20.61009Z","iopub.status.idle":"2022-05-06T05:26:21.073898Z","shell.execute_reply.started":"2022-05-06T05:26:20.609978Z","shell.execute_reply":"2022-05-06T05:26:21.072743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q efficientnet","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:21.076471Z","iopub.execute_input":"2022-05-06T05:26:21.076749Z","iopub.status.idle":"2022-05-06T05:26:32.070931Z","shell.execute_reply.started":"2022-05-06T05:26:21.07672Z","shell.execute_reply":"2022-05-06T05:26:32.070027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nimport math, re, os\nimport random\nimport plotly.express as px\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n#tensorflow.keras.backend as K\n\nfrom kaggle_datasets import KaggleDatasets\nimport efficientnet.tfkeras as efn\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nfrom tensorflow.keras.applications import DenseNet201\n\nprint(\"Tensorflow version \" + tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:32.073179Z","iopub.execute_input":"2022-05-06T05:26:32.073573Z","iopub.status.idle":"2022-05-06T05:26:41.68901Z","shell.execute_reply.started":"2022-05-06T05:26:32.073522Z","shell.execute_reply":"2022-05-06T05:26:41.688359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Defining Distribution Strategy","metadata":{}},{"cell_type":"markdown","source":"We'll use the distribution strategy when we create our neural network model. Then, TensorFlow will distribute the training among the eight TPU cores by creating eight different replicas of the model, one for each core.","metadata":{}},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    \n    # TPUs are network-connected accelerators. TPUClusterResolver() locates them on the network.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    # Configuring the tpu for this object to contain the necessary distributed training code that will work on TPUs with their 8 compute cores\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    # Defining TPUStrategy by instantiating the model in the scope of the strategy. \n    # This creates the model on the TPU.\n    # Model size is constrained by the TPU RAM only, not by the amount of memory available on the VM running your Python code. \n    # Model creation and model training use the usual Keras APIs.\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:41.690155Z","iopub.execute_input":"2022-05-06T05:26:41.690748Z","iopub.status.idle":"2022-05-06T05:26:55.572555Z","shell.execute_reply.started":"2022-05-06T05:26:41.690712Z","shell.execute_reply":"2022-05-06T05:26:55.571483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Loading the Competition Data","metadata":{}},{"cell_type":"markdown","source":"### Getting the GCS Path\n\nWe will retrieve the GCS path for this competition's dataset from the Google Cloud Storage bucket.","metadata":{}},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\n# Printing the GCS path for this dataset\nprint(GCS_DS_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:55.576352Z","iopub.execute_input":"2022-05-06T05:26:55.576688Z","iopub.status.idle":"2022-05-06T05:26:56.07281Z","shell.execute_reply.started":"2022-05-06T05:26:55.576654Z","shell.execute_reply":"2022-05-06T05:26:56.071873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the  Data\n\nWhen used with TPUs, datasets are often serialized into TFRecords. \nThis is a format convenient for distributing data to each of the TPUs cores. ","metadata":{}},{"cell_type":"markdown","source":"Source: https://www.kaggle.com/docs/tpu","metadata":{}},{"cell_type":"code","source":"''' TPUs are equipped with 128GB of high-speed memory allowing larger batches, larger models and also larger training inputs. \nIn the code below, we can use 512x512 px input images, also provided in the dataset, and see the TPU v3-8 handle them easily.'''\nIMAGE_SIZE = [512, 512]\n\nGCS_PATH = GCS_DS_PATH + '/tfrecords-jpeg-512x512'\nAUTO = tf.data.experimental.AUTOTUNE\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') \n\nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 103\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:56.074319Z","iopub.execute_input":"2022-05-06T05:26:56.074577Z","iopub.status.idle":"2022-05-06T05:26:56.334214Z","shell.execute_reply.started":"2022-05-06T05:26:56.074546Z","shell.execute_reply":"2022-05-06T05:26:56.333201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_FILENAMES","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:56.335789Z","iopub.execute_input":"2022-05-06T05:26:56.336353Z","iopub.status.idle":"2022-05-06T05:26:56.344791Z","shell.execute_reply.started":"2022-05-06T05:26:56.336314Z","shell.execute_reply":"2022-05-06T05:26:56.34414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    '''experimental_deterministic = False disables data order enforcement. \n    We will be shuffling the data anyway so order is not important. \n    With this setting the API can use any TFRecord as soon as it is streamed in.'''\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed.\n\n    # num_parallel_reads=AUTO instructs the API to read from multiple files if available. It figures out how many automatically.\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files.\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:56.34621Z","iopub.execute_input":"2022-05-06T05:26:56.346544Z","iopub.status.idle":"2022-05-06T05:26:56.361159Z","shell.execute_reply.started":"2022-05-06T05:26:56.346516Z","shell.execute_reply":"2022-05-06T05:26:56.360278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation Section","metadata":{}},{"cell_type":"markdown","source":"Note 9 ðŸ˜€\nUse additional data, tuning6, private dataset\nInspired by Dmitry's notebook here and Araik's notebook here\nSee also external data and how to use them and Kirill's tf_flower_photo_tfrec dataset","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:56.362694Z","iopub.execute_input":"2022-05-06T05:26:56.362974Z","iopub.status.idle":"2022-05-06T05:26:57.142455Z","shell.execute_reply.started":"2022-05-06T05:26:56.362941Z","shell.execute_reply":"2022-05-06T05:26:57.14144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_DS_PATH_EXT = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')\n\n# External data\nGCS_PATH_SELECT_EXT = {\n    192: '/tfrecords-jpeg-192x192',\n    224: '/tfrecords-jpeg-224x224',\n    331: '/tfrecords-jpeg-331x331',\n    512: '/tfrecords-jpeg-512x512'\n}\nGCS_PATH_EXT = GCS_PATH_SELECT_EXT[IMAGE_SIZE[0]]\n\nIMAGENET_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/imagenet' + GCS_PATH_EXT + '/*.tfrec')\nINATURELIST_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/inaturalist' + GCS_PATH_EXT + '/*.tfrec')\nOPENIMAGE_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/openimage' + GCS_PATH_EXT + '/*.tfrec')\nOXFORD_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/oxford_102' + GCS_PATH_EXT + '/*.tfrec')\nTENSORFLOW_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/tf_flowers' + GCS_PATH_EXT + '/*.tfrec')\n\nADDITIONAL_TRAINING_FILENAMES = IMAGENET_FILES + INATURELIST_FILES + OPENIMAGE_FILES + OXFORD_FILES + TENSORFLOW_FILES  \n\nTRAINING_FILENAMES = TRAINING_FILENAMES + ADDITIONAL_TRAINING_FILENAMES","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:57.14402Z","iopub.execute_input":"2022-05-06T05:26:57.144731Z","iopub.status.idle":"2022-05-06T05:26:57.918772Z","shell.execute_reply.started":"2022-05-06T05:26:57.144686Z","shell.execute_reply":"2022-05-06T05:26:57.917741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note 8 ðŸ˜€\nPerform data augmentation, tuning4\nInspired by Dmitry's notebook here","metadata":{}},{"cell_type":"code","source":"#tuning4\nSEED = 2020\n\ndef random_blockout(img, sl=0.1, sh=0.2, rl=0.4):\n    p=random.random()\n    if p>=0.25:\n        w, h, c = IMAGE_SIZE[0], IMAGE_SIZE[1], 3\n        origin_area = tf.cast(h*w, tf.float32)\n\n        e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n        e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh / rl)), tf.int32)\n\n        e_height_h = tf.minimum(e_size_h, h)\n        e_width_h = tf.minimum(e_size_h, w)\n\n        erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n        erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n\n        erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n        erase_area = tf.cast(erase_area, tf.uint8)\n\n        pad_h = h - erase_height\n        pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n        pad_bottom = pad_h - pad_top\n\n        pad_w = w - erase_width\n        pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n        pad_right = pad_w - pad_left\n\n        erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n        erase_mask = tf.squeeze(erase_mask, axis=0)\n        erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n\n        return tf.cast(erased_img, img.dtype)\n    else:\n        return tf.cast(img, img.dtype)\n\n    \ndef data_augment_v2(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the next function (below), this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    \n    flag = random.randint(1,3)\n    coef_1 = random.randint(70, 90) * 0.01\n    coef_2 = random.randint(70, 90) * 0.01\n    \n    if flag == 1:\n        image = tf.image.random_flip_left_right(image, seed=SEED)\n    elif flag == 2:\n        image = tf.image.random_flip_up_down(image, seed=SEED)\n    else:\n        image = tf.image.random_crop(image, [int(IMAGE_SIZE[0]*coef_1), int(IMAGE_SIZE[0]*coef_2), 3],seed=SEED)\n        \n    image = random_blockout(image)\n    \n    return image, label ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:57.920427Z","iopub.execute_input":"2022-05-06T05:26:57.921059Z","iopub.status.idle":"2022-05-06T05:26:57.942356Z","shell.execute_reply.started":"2022-05-06T05:26:57.921017Z","shell.execute_reply":"2022-05-06T05:26:57.940998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perform data augmentation, tuning7\nInspired by Xuanzhi Huang and Rahul Paul's notebook here","metadata":{}},{"cell_type":"code","source":"import tensorflow_addons as tfa\n\n# Randomly make some changes to the images and return the new images and labels\ndef data_augment_v3(image, label):\n        \n    # Set seed for data augmentation\n    seed = 100\n    \n    # Randomly resize and then crop images\n    image = tf.image.resize(image, [720, 720])\n    image = tf.image.random_crop(image, [512, 512, 3], seed = seed)\n\n    # Randomly reset brightness of images\n    image = tf.image.random_brightness(image, 0.6, seed = seed)\n    \n    # Randomly reset saturation of images\n    image = tf.image.random_saturation(image, 3, 5, seed = seed)\n        \n    # Randomly reset contrast of images\n    image = tf.image.random_contrast(image, 0.3, 0.5, seed = seed)\n\n    # Randomly reset hue of images, but this will make the colors really weird, which we think will not happen\n    # in common photography\n    # image = tf.image.random_hue(image, 0.5, seed = seed)\n    \n    # Blur images\n    image = tfa.image.mean_filter2d(image, filter_shape = 10)\n    \n    # Randomly flip images\n    image = tf.image.random_flip_left_right(image, seed = seed)\n    image = tf.image.random_flip_up_down(image, seed = seed)\n    \n    # Fail to rotate and transform images due to some bug in TensorFlow\n    # angle = random.randint(0, 180)\n    # image = tfa.image.rotate(image, tf.constant(np.pi * angle / 180))\n    # image = tfa.image.transform(image, [1.0, 1.0, -250, 0.0, 1.0, 0.0, 0.0, 0.0])\n    \n    return image, label","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:57.944755Z","iopub.execute_input":"2022-05-06T05:26:57.94523Z","iopub.status.idle":"2022-05-06T05:26:58.120053Z","shell.execute_reply.started":"2022-05-06T05:26:57.945177Z","shell.execute_reply":"2022-05-06T05:26:58.119334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Data Pipeline","metadata":{}},{"cell_type":"markdown","source":"Create Data Pipelines\nIn this final step we'll use the tf.data API to define an efficient data pipeline for each of the training, validation, and test splits.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def data_augment(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the next function (below), this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    #image = tf.image.random_saturation(image, 0, 2)\n    return image, label   \n\n# Additional DATA Augmentation code (not used as of now for version-9)\ndef data_augment_contrast( image, label ):    \n    image = tf.image.random_contrast( image, 0.7, 1 )     \n    image = tf.image.random_flip_up_down( image )    \n    image = tf.image.random_flip_left_right( image )    \n    return image, label\n\ndef data_augment_brightness( image, label ):    \n    image = tf.image.random_brightness( image, 1 )    \n    image = tf.image.random_flip_up_down( image )    \n    image = tf.image.random_flip_left_right( image )    \n    return image, label\n\ndef data_augment_saturation( image, label ):    \n    image = tf.image.random_saturation( image, 5, 10)    \n    image = tf.image.random_flip_up_down( image )    \n    image = tf.image.random_flip_left_right( image )    \n    return image, label\n\ndef data_augment_crop(image, label):    \n    image = tf.image.random_crop( value = image, size = [180, 180, 3] )    \n    image = tf.image.random_flip_up_down( image )    \n    image = tf.image.random_flip_left_right( image )    \n    image = tf.image.resize( image, [*IMAGE_SIZE] )    \n    return image, label\n\n# Old version of get_training_dataset, changed below by Rodrigo\n'''def get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO) #tuning4\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset'''\n\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO) #tuning4    \n    #ds_contrast = dataset.map( data_augment_contrast, num_parallel_calls = AUTO)\n    #ds_brightness = dataset.map( data_augment_brightness, num_parallel_calls = AUTO)\n    #ds_saturation = dataset.map( data_augment_saturation, num_parallel_calls = AUTO)\n    #ds_crop = dataset.map( data_augment_crop, num_parallel_calls = AUTO)\n    #dataset = dataset.concatenate( ds_contrast ).concatenate( ds_brightness ).concatenate( ds_saturation ).concatenate( ds_crop )    \n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\n\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\n\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:58.122052Z","iopub.execute_input":"2022-05-06T05:26:58.12272Z","iopub.status.idle":"2022-05-06T05:26:58.149191Z","shell.execute_reply.started":"2022-05-06T05:26:58.122672Z","shell.execute_reply":"2022-05-06T05:26:58.148083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Original Data: Dataset: 12753 training images, 3712 validation images, 7382 unlabeled test images\nAdditional Data: Dataset: 68094 training images, 3712 validation images, 7382 unlabeled test images\nThis next cell will create the datasets that we'll use with Keras during training and inference. Notice how we scale the size of the batches to the number of TPU cores.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note 3 ðŸ˜€\n1. To go fast on a TPU, increase the batch size. The rule of thumb is to use batches of 128 elements per core (ex: batch size of 128*8=1024 for a TPU with 8 cores). At this size, the 128x128 hardware matrix multipliers of the TPU (see hardware section below) are most likely to be kept busy. You start seeing interesting speedups from a batch size of 8 per core though. In the sample above, the batch size is scaled with the core count through this line of code:\nBATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\nSource: https://www.kaggle.com/docs/tpu","metadata":{}},{"cell_type":"code","source":"strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:58.154556Z","iopub.execute_input":"2022-05-06T05:26:58.154891Z","iopub.status.idle":"2022-05-06T05:26:58.16803Z","shell.execute_reply.started":"2022-05-06T05:26:58.154857Z","shell.execute_reply":"2022-05-06T05:26:58.166966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"16 * strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:58.169408Z","iopub.execute_input":"2022-05-06T05:26:58.169882Z","iopub.status.idle":"2022-05-06T05:26:58.177301Z","shell.execute_reply.started":"2022-05-06T05:26:58.16974Z","shell.execute_reply":"2022-05-06T05:26:58.176356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the batch size. This will be 16 with TPU off and 128 (=16*8) with TPU on\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync #See Note 3.1 above ðŸ˜€\n\nds_train = get_training_dataset()\nds_valid = get_validation_dataset()\nds_test = get_test_dataset()\n\nprint(\"Training:\", ds_train)\nprint (\"Validation:\", ds_valid)\nprint(\"Test:\", ds_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:58.178917Z","iopub.execute_input":"2022-05-06T05:26:58.179433Z","iopub.status.idle":"2022-05-06T05:26:58.577239Z","shell.execute_reply.started":"2022-05-06T05:26:58.17939Z","shell.execute_reply":"2022-05-06T05:26:58.576142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These datasets are tf.data.Dataset objects. You can think about a dataset in TensorFlow as a stream of data records. The training and validation sets are streams of (image, label) pairs.","metadata":{}},{"cell_type":"code","source":"np.set_printoptions(threshold=15, linewidth=80)\n\nprint(\"Training data shapes:\")\nfor image, label in ds_train.take(3):\n    print(image.numpy().shape, label.numpy().shape) #See Note 3.1 above ðŸ˜€\nprint(\"Training data label examples:\", label.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:26:58.578525Z","iopub.execute_input":"2022-05-06T05:26:58.578763Z","iopub.status.idle":"2022-05-06T05:27:06.142725Z","shell.execute_reply.started":"2022-05-06T05:26:58.578735Z","shell.execute_reply":"2022-05-06T05:27:06.141981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test set is a stream of (image, idnum) pairs; idnum here is the unique identifier given to the image that we'll use later when we make our submission as a csv file.","metadata":{}},{"cell_type":"code","source":"print(\"Test data shapes:\")\nfor image, idnum in ds_test.take(3):\n    print(image.numpy().shape, idnum.numpy().shape) #See Note 3.1 above ðŸ˜€\nprint(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:27:06.144096Z","iopub.execute_input":"2022-05-06T05:27:06.144517Z","iopub.status.idle":"2022-05-06T05:27:10.165944Z","shell.execute_reply.started":"2022-05-06T05:27:06.144471Z","shell.execute_reply":"2022-05-06T05:27:10.164753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Exploring the Data","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object: # binary string in this case,these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n        # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], \n                                'OK' if correct else 'NO', \n                                u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n    \ndef display_batch_of_images(databatch, predictions=None, display_mismatches_only=False):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)//rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        if display_mismatches_only:\n            if predictions[i] != label:\n                subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n        else:        \n            subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n\ndef display_training_curves_v2(training, validation, learning_rate_list, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title, color='b')\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.', 'learning rate'])        \n    \n    ax2 = ax.twinx()\n    ax2.plot(learning_rate_list, 'g-')\n    ax2.set_ylabel('learning rate', color='g')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:27:10.168508Z","iopub.execute_input":"2022-05-06T05:27:10.168764Z","iopub.status.idle":"2022-05-06T05:27:10.197733Z","shell.execute_reply.started":"2022-05-06T05:27:10.168735Z","shell.execute_reply":"2022-05-06T05:27:10.196476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images.","metadata":{}},{"cell_type":"code","source":"ds_iter = iter(ds_train.unbatch().batch(20))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:27:10.199299Z","iopub.execute_input":"2022-05-06T05:27:10.199774Z","iopub.status.idle":"2022-05-06T05:27:10.226546Z","shell.execute_reply.started":"2022-05-06T05:27:10.199727Z","shell.execute_reply":"2022-05-06T05:27:10.225422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the Python next function to pop out the next batch in the stream and display it with the helper function.","metadata":{}},{"cell_type":"code","source":"one_batch = next(ds_iter)\ndisplay_batch_of_images(one_batch)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:27:10.230553Z","iopub.execute_input":"2022-05-06T05:27:10.231217Z","iopub.status.idle":"2022-05-06T05:27:15.435373Z","shell.execute_reply.started":"2022-05-06T05:27:10.23116Z","shell.execute_reply":"2022-05-06T05:27:15.43447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By defining ds_iter and one_batch in separate cells, you only need to rerun the cell above to see a new batch of images.","metadata":{}},{"cell_type":"markdown","source":"tuning7, show a sample of data augmented","metadata":{}},{"cell_type":"code","source":"row = 3\ncol = 4\nall_elements = get_training_dataset().unbatch()\none_element = tf.data.Dataset.from_tensors(next(iter(all_elements)))\n# Map the images to the data augmentation function for image processing\naugmented_element = one_element.repeat().map(data_augment).batch(row * col)\n\nfor (img, label) in augmented_element:\n    plt.figure(figsize = (15, int(15 * row / col)))\n    for j in range(row * col):\n        plt.subplot(row, col, j + 1)\n        plt.axis('off')\n        plt.imshow(img[j, ])\n    plt.show()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:27:15.43689Z","iopub.execute_input":"2022-05-06T05:27:15.437169Z","iopub.status.idle":"2022-05-06T05:27:19.012402Z","shell.execute_reply.started":"2022-05-06T05:27:15.43714Z","shell.execute_reply":"2022-05-06T05:27:19.011301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tuning7, show a sample of data augmented v2","metadata":{}},{"cell_type":"code","source":"# Map the images to the data augmentation function for image processing\naugmented_element = one_element.repeat().map(data_augment_v2).batch(row * col)\n\nfor (img, label) in augmented_element:\n    plt.figure(figsize = (15, int(15 * row / col)))\n    for j in range(row * col):\n        plt.subplot(row, col, j + 1)\n        plt.axis('off')\n        plt.imshow(img[j, ])\n    plt.show()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:27:19.013889Z","iopub.execute_input":"2022-05-06T05:27:19.014569Z","iopub.status.idle":"2022-05-06T05:27:22.240621Z","shell.execute_reply.started":"2022-05-06T05:27:19.014514Z","shell.execute_reply":"2022-05-06T05:27:22.239931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tuning7, show a sample of data augmented v3","metadata":{}},{"cell_type":"code","source":"# Map the images to the data augmentation function for image processing\naugmented_element = one_element.repeat().map(data_augment_v3).batch(row * col)\n\nfor (img, label) in augmented_element:\n    plt.figure(figsize = (15, int(15 * row / col)))\n    for j in range(row * col):\n        plt.subplot(row, col, j + 1)\n        plt.axis('off')\n        plt.imshow(img[j, ])\n    plt.show()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:27:22.241819Z","iopub.execute_input":"2022-05-06T05:27:22.242253Z","iopub.status.idle":"2022-05-06T05:27:30.65704Z","shell.execute_reply.started":"2022-05-06T05:27:22.242221Z","shell.execute_reply":"2022-05-06T05:27:30.655973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From here, Rosa's notebook","metadata":{}},{"cell_type":"code","source":"# Need this line so Google will recite some incantations\n# for Turing to magically load the model onto the TPU\nwith strategy.scope():\n    enet = efn.EfficientNetB7(\n        input_shape=(512, 512, 3),\n        weights='noisy-student',\n        include_top=False)\n\n    model = tf.keras.Sequential([\n        enet,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')])\n        \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy'])\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:27:30.658773Z","iopub.execute_input":"2022-05-06T05:27:30.659192Z","iopub.status.idle":"2022-05-06T05:28:18.890476Z","shell.execute_reply.started":"2022-05-06T05:27:30.659102Z","shell.execute_reply":"2022-05-06T05:28:18.889101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nEPOCHS = 20","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:28:18.891945Z","iopub.execute_input":"2022-05-06T05:28:18.892235Z","iopub.status.idle":"2022-05-06T05:28:18.898015Z","shell.execute_reply.started":"2022-05-06T05:28:18.892204Z","shell.execute_reply":"2022-05-06T05:28:18.896792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Custom LR Schedule","metadata":{}},{"cell_type":"code","source":"LR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:28:18.899858Z","iopub.execute_input":"2022-05-06T05:28:18.900223Z","iopub.status.idle":"2022-05-06T05:28:19.162502Z","shell.execute_reply.started":"2022-05-06T05:28:18.900187Z","shell.execute_reply":"2022-05-06T05:28:19.161037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# watch out for overfitting!\nSKIP_VALIDATION = True\nif SKIP_VALIDATION:\n    TRAINING_FILENAMES = TRAINING_FILENAMES + VALIDATION_FILENAMES","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:28:19.164352Z","iopub.execute_input":"2022-05-06T05:28:19.164783Z","iopub.status.idle":"2022-05-06T05:28:19.171022Z","shell.execute_reply.started":"2022-05-06T05:28:19.164735Z","shell.execute_reply":"2022-05-06T05:28:19.170023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\n\n#scheduler = tf.keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\nwith strategy.scope():\n    history = model.fit(\n        get_training_dataset(), \n        steps_per_epoch=STEPS_PER_EPOCH,\n        epochs=EPOCHS,\n        callbacks=[lr_callback],\n        validation_data=None if SKIP_VALIDATION else get_validation_dataset())","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:28:19.172589Z","iopub.execute_input":"2022-05-06T05:28:19.173123Z","iopub.status.idle":"2022-05-06T08:09:14.568594Z","shell.execute_reply.started":"2022-05-06T05:28:19.173075Z","shell.execute_reply":"2022-05-06T08:09:14.567669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not SKIP_VALIDATION:\n    display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n    display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T08:09:14.570225Z","iopub.execute_input":"2022-05-06T08:09:14.570586Z","iopub.status.idle":"2022-05-06T08:09:14.577155Z","shell.execute_reply.started":"2022-05-06T08:09:14.570554Z","shell.execute_reply":"2022-05-06T08:09:14.576098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training 2","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    rnet = DenseNet201(\n        input_shape=(512, 512, 3),\n        weights='imagenet',\n        include_top=False\n    )\n\n    model2 = tf.keras.Sequential([\n        rnet,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])\n        \n    model2.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy'])\nmodel2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T08:09:14.578696Z","iopub.execute_input":"2022-05-06T08:09:14.579171Z","iopub.status.idle":"2022-05-06T08:09:57.591725Z","shell.execute_reply.started":"2022-05-06T08:09:14.579135Z","shell.execute_reply":"2022-05-06T08:09:57.590609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    history2 = model2.fit(\n        get_training_dataset(), \n        steps_per_epoch=STEPS_PER_EPOCH,\n        epochs=EPOCHS, \n        callbacks=[lr_callback],\n        validation_data=None if SKIP_VALIDATION else get_validation_dataset())","metadata":{"execution":{"iopub.status.busy":"2022-05-06T08:09:57.59341Z","iopub.execute_input":"2022-05-06T08:09:57.593989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not SKIP_VALIDATION:\n    display_training_curves(history2.history['loss'], history2.history['val_loss'], 'loss', 211)\n    display_training_curves(history2.history['sparse_categorical_accuracy'], history2.history['val_sparse_categorical_accuracy'], 'accuracy', 212)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding best alpha\nOur final model is just mix of two presented above. In the first commit it was arithmetic mean (alpha = 0.5). Note that using validation data as training will fit your model with accuracy equal 1.0. Thus formula presented below of linear combination of models will work only with validation data:\n\nprob = alpha prob(model) + (1 - alpha) prob(model2)","metadata":{}},{"cell_type":"code","source":"if not SKIP_VALIDATION:\n    cmdataset = get_validation_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and labels, order matters.\n    images_ds = cmdataset.map(lambda image, label: image)\n    labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n    cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # get everything as one batch\n    m = model.predict(images_ds)\n    m2 = model2.predict(images_ds)\n    scores = []\n    for alpha in np.linspace(0,1,100):\n        cm_probabilities = alpha*m+(1-alpha)*m2\n        cm_predictions = np.argmax(cm_probabilities, axis=-1)\n        scores.append(f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro'))\n        \n    print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n    print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)\n    plt.plot(scores)\n    best_alpha = np.argmax(scores)/100\n    cm_probabilities = best_alpha*m+(1-best_alpha)*m2\n    cm_predictions = np.argmax(cm_probabilities, axis=-1)\nelse:\n    best_alpha = 0.51","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_alpha)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mismatches on a validation data","metadata":{}},{"cell_type":"code","source":"if not SKIP_VALIDATION:\n    \n    cmdataset_with_id = get_validation_dataset_with_id(ordered=True)\n    ids_ds = cmdataset_with_id.map(lambda image, label, idnum: idnum).unbatch()\n    ids = next(iter(ids_ds.batch(NUM_VALIDATION_IMAGES))).numpy().astype('U') # get everything as one batch\n\n    val_batch = iter(cmdataset.unbatch().batch(1))\n    noip = sum(cm_predictions!=cm_correct_labels)\n    print('Number of incorrect predictions: ' + str(noip) + ' ('+str(round(noip/NUM_VALIDATION_IMAGES*100,1))+'%)')\n    for fi in range(NUM_VALIDATION_IMAGES):\n        x = next(val_batch)\n        if cm_predictions[fi] != cm_correct_labels[fi]:\n            print(\"Image id: '\" + ids[fi] + \"'\")\n            display_batch_of_images(x,np.array([cm_predictions[fi]]),figsize = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion matrix","metadata":{}},{"cell_type":"code","source":"if not SKIP_VALIDATION:\n    cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n    score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    #cmat = (cmat.T / cmat.sum(axis=1)).T # normalized\n    display_confusion_matrix(cmat, score, precision, recall)\n    print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictions","metadata":{}},{"cell_type":"code","source":"test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = best_alpha*model.predict(test_images_ds) + (1-best_alpha)*model2.predict(test_images_ds)\npredictions = np.argmax(probabilities, axis=-1)\nprint(predictions)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}